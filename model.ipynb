{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e937dd-3fd9-45c5-bdef-dc89ea1bc76e",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e96b7-6ae6-49d0-bec5-27e1e17fec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For feature selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For model development\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "# For model interpretation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c00a8-272c-4205-9dd4-bd5ce0350f26",
   "metadata": {},
   "source": [
    "# 2. Data loading, cleaning and preprocessing\n",
    "\n",
    "Preprocessing strategy: Multiplicative Scatter Correction followed by Standardisation, decided by trial and error (not shown here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bff78f-fdd3-4b60-9e90-0d91d925f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and processing data from Excel sheets...\")\n",
    "\n",
    "# File path and sheet names\n",
    "excel_file = 'Data for modelling.xlsx'\n",
    "sheet_names = [\"Condition 1 Day 0\", \"Condition 2 Day 0\", \"Condition 3 Day 0\"]\n",
    "\n",
    "# Create lists to store features and labels, and build feature names\n",
    "features_list = []\n",
    "labels_list = []\n",
    "feature_names = [f\"MSC_{i+1}\" for i in range(125)] # there are 125 wavelengths in each original spectrum\n",
    "\n",
    "# Define functions for data cleaning and preprocessing\n",
    "\n",
    "def clean_spectral_data(df_spectra):\n",
    "    \"\"\"\n",
    "    Remove unwanted patterns (e.g., '+C19') from spectral data.\n",
    "    \"\"\"\n",
    "    return df_spectra.apply(lambda col: col.astype(str).str.replace(r'\\+C\\d+', '', regex=True))\n",
    "\n",
    "def compute_msc(spectra):\n",
    "    \"\"\"\n",
    "    Apply Multiplicative Scatter Correction (MSC)\n",
    "    \"\"\"\n",
    "    mean_spectrum = np.mean(spectra, axis=0)\n",
    "    corrected_data = np.zeros_like(spectra)\n",
    "    for i in range(spectra.shape[0]):\n",
    "        fit = np.polyfit(mean_spectrum, spectra[i, :], 1, full=True)\n",
    "        corrected_data[i, :] = (spectra[i, :] - fit[0][1]) / fit[0][0]\n",
    "    return corrected_data\n",
    "\n",
    "for idx, sheet in enumerate(sheet_names):\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet, header=None)\n",
    "    \n",
    "    # Spectral data starts from second row and first column (wavelengths in the first row)\n",
    "    spectra_df = df.iloc[1:, 0:].copy()\n",
    "    spectra_df = clean_spectral_data(spectra_df)\n",
    "    X_original = spectra_df.astype(np.float32).values\n",
    "    \n",
    "    # Apply MSC followed by Standardisation\n",
    "    X_msc = compute_msc(X_original)\n",
    "    X_std_msc = StandardScaler().fit_transform(X_msc)\n",
    "    \n",
    "    features_list.append(X_std_msc)\n",
    "    labels_list.append(np.full(X_original.shape[0], idx))\n",
    "    \n",
    "# Concatenate data from all sheets\n",
    "X_all = np.vstack(features_list)\n",
    "y_all = np.concatenate(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb130cd5-6508-4a25-be47-3d4e0103c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sheet '{sheet}': Original shape: {X_original.shape}, Processed shape: {X_std_msc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a7e16-1332-4a28-ab63-00e38fa32209",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Combined data shape: {X_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c53cb-1bc2-49a8-a74b-35511264c5f8",
   "metadata": {},
   "source": [
    "# 3. Split data into Training, Validation and Test sets\n",
    "\n",
    "Training and Validation sets are used for \n",
    "(i)  feature selection (Step 4), and\n",
    "(ii) model building/hyperparameter tuning (Step 5)\n",
    "\n",
    "Test set is held out for Model Evaluation in Step 6 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcc028-aff2-4fd9-8f87-f45fdba68f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSplitting data into training, validation, and test sets...\")\n",
    "\n",
    "# Reserve 15% for testing. Fix random seed for reproducibility. Change to generate new folds.\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.15,\n",
    "    stratify=y_all,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# Split the remaining 85% into training and validation sets (15% of 85% ~ 17.65% total).\n",
    "val_fraction = 0.15 / 0.85\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=val_fraction,\n",
    "    stratify=y_temp,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c641a70f-053d-4792-9138-9a07ddb8556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0092fce-f24b-478c-9ac4-e6b880319aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation set shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b787fe9-a544-4dda-bf11-5913c77c09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b729c7-7b07-41cf-a9e0-f04c5ffecd76",
   "metadata": {},
   "source": [
    "# 4. Feature selection\n",
    "\n",
    "Using Recursive Feature Elimination and Random Forest Classifier.\n",
    "Only Training and Validation sets are used; Test set is not seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6ce93-2d22-447f-a60e-5a4b48fbc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPerforming RFE to select the top features...\")\n",
    "\n",
    "# Define the base estimator for RFE. Fix random state for reproducibility\n",
    "rfe_estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Setting verbose=1 will show progress information during RFE\n",
    "selector = RFE(\n",
    "    estimator=rfe_estimator,\n",
    "    n_features_to_select=30,  # Select top 30 features, change if needed\n",
    "    step=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit on the training set\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform train, validation, and test sets\n",
    "X_train_rfe = selector.transform(X_train)\n",
    "X_val_rfe   = selector.transform(X_val)\n",
    "X_test_rfe  = selector.transform(X_test)\n",
    "\n",
    "print(f\"After RFE, number of selected features: {X_train_rfe.shape[1]}\")\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_feature_indices = np.where(selector.support_)[0]\n",
    "\n",
    "print(\"Selected feature indices:\", selected_feature_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7cab1-eba7-437d-bb69-769efb4c8eed",
   "metadata": {},
   "source": [
    "# 4.1 Optional: Save the selected feature indices for later retrieval without re-selected features, and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23d397-5e03-4b4f-a6cb-c4af42e3ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory to save and the name of the file\n",
    "# output_dir = 'feature selection'\n",
    "# np.save(os.path.join(output_dir, 'RF_selected_features_v1.npy'), selected_feature_indices)\n",
    "\n",
    "# print(f\"Selected feature indices saved to '{output_dir}/RF_selected_features_v1.npy'.\")\n",
    "\n",
    "# Load the selected feature indices from file\n",
    "\n",
    "# print(\"\\nLoading selected features\")\n",
    "\n",
    "# selected_features = np.load(\"RF_selected_features_v1.npy\")\n",
    "\n",
    "# Transform train, validation, and test sets using the selected features\n",
    "# X_train_rfe = X_train[:, selected_features]\n",
    "# X_val_rfe   = X_val[:, selected_features]\n",
    "# X_test_rfe  = X_test[:, selected_features]\n",
    "\n",
    "# print(f\"After loading selected features, number of features: {X_train_rfe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1532d6-cffe-40ea-82f5-e5d8ea7834e7",
   "metadata": {},
   "source": [
    "# 5. Model development\n",
    "\n",
    "Train Artificial Neural Network on training set only and Monitor performance  using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a31742-4600-4051-a36a-9bc452e10f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds for reproducibility. Can be changed to try different model initialisation weights\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "num_classes = 3\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_val_cat   = to_categorical(y_val, num_classes)\n",
    "final_input_dim = X_train_rfe.shape[1]\n",
    "\n",
    "# Set and tune hyperparameters\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "learning_rate = 0.0033\n",
    "num_hidden_layer_nodes = 3\n",
    "\n",
    "# Build the initial model\n",
    "model = Sequential([\n",
    "    Input(shape=(final_input_dim,)),\n",
    "    Dense(num_hidden_layer_nodes, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n--- Training on Training Set Only ---\")\n",
    "\n",
    "# Fit training data to the model and store the training information for later analysis\n",
    "history = model.fit(\n",
    "    X_train_rfe, y_train_cat,\n",
    "    validation_data=(X_val_rfe, y_val_cat),\n",
    "    epochs=epochs, batch_size=batch_size, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc43d92-cfeb-495f-81cb-4d4f8a17859e",
   "metadata": {},
   "source": [
    "# 6. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f83307-70eb-4c3b-a69a-c7521ac9584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions of classes on the test set\n",
    "pred_test = np.argmax(model.predict(X_test_rfe), axis=1)\n",
    "\n",
    "# Also get the predictions on the training set for recording later\n",
    "pred_train = np.argmax(model.predict(X_train_rfe), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8c274-cb1f-4efe-a5b8-bbd3829162ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFinal Test Accuracy: {acc_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a26fc-270c-4810-9b62-f45396386c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e21b30-81e4-4079-a2fd-89f045d5ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f54af-c970-45fd-aece-af8b8769b7ff",
   "metadata": {},
   "source": [
    "# 7. Extract Principal Components (PCs) of Hidden Layer Activations and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6f95e-f5a7-48b5-ae67-eb0592843792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComputing and Plotting PCs of Hidden Layer Activations...\")\n",
    "\n",
    "# Combine full dataset (training, validation, and test) in RFE-space\n",
    "X_full_rfe = np.vstack((X_train_rfe, X_val_rfe, X_test_rfe))\n",
    "y_full     = np.concatenate((y_train, y_val, y_test))\n",
    "\n",
    "# Force a dummy pass so the model's input is defined.\n",
    "dummy_input = np.zeros((1, final_input_dim))\n",
    "_ = model(dummy_input)\n",
    "\n",
    "# Extract the hidden layer activations using the first input tensor.\n",
    "hidden_layer_model = tf.keras.Model(\n",
    "    inputs=model.inputs[0],\n",
    "    outputs=model.layers[-2].output\n",
    ")\n",
    "hidden_activations = hidden_layer_model.predict(X_full_rfe)  # Shape: (n_samples, 5)\n",
    "\n",
    "# Perform PCA on the hidden activations to reduce to 3 principal components.\n",
    "pca = PCA(n_components=3)\n",
    "pcs = pca.fit_transform(hidden_activations)\n",
    "\n",
    "# Define condition names and their colours for plotting\n",
    "condition_names = {0: \"Condition 1\", 1: \"Condition 2\", 2: \"Condition 3\"}\n",
    "colors = ['blue', 'orange', 'green']\n",
    "\n",
    "# Plot PC1 vs. PC2, PC2 vs. PC3, PC3 vs. PC1\n",
    "pairs = [(0, 1), (1, 2), (2, 0)]\n",
    "\n",
    "for x_idx, y_idx in pairs: # x_idx refers to the PC being plotted on the x-axis, y_idx refers to the PC being plotted on the y-axis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for cond in range(num_classes):\n",
    "        cond_idx = np.where(y_full == cond)[0]\n",
    "        x_scores = pcs[cond_idx, x_idx]\n",
    "        y_scores = pcs[cond_idx, y_idx]\n",
    "        \n",
    "        plt.scatter(x_scores, y_scores, color=colors[cond], alpha=0.6, label=condition_names[cond])\n",
    "    \n",
    "    plt.xlabel(f\"PC{x_idx+1} Score\")\n",
    "    plt.ylabel(f\"PC{y_idx+1} Score\")\n",
    "    plt.title(f\"PC{x_idx+1} vs PC{y_idx+1} of Hidden Layer Activations (Full Dataset)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f58201-2a21-4413-994d-8bbd8cfc5617",
   "metadata": {},
   "source": [
    "# 8. Compute Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98926012-509e-46bb-ba0c-56a6122d8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasClassifierWrapper:\n",
    "    \"\"\"Wrapper for Keras model to enable use with permutation_importance.\"\"\"\n",
    "    def __init__(self, keras_model):\n",
    "        self.model = keras_model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts class labels using the trained model.\"\"\"\n",
    "        pred_probabilities = self.model.predict(X)    # Get the predicted probabilities\n",
    "        return np.argmax(pred_probabilities, axis=1)  # Return the class with the highest probability i.e., predicted class\n",
    "\n",
    "print(\"\\nComputing permutation importance...\")\n",
    "\n",
    "# Wrap the trained model for use with permutation importance\n",
    "wrapper_clf = KerasClassifierWrapper(model)\n",
    "\n",
    "# Calculate permutation importance with accuracy as the scoring method\n",
    "perm_imp_class = permutation_importance(\n",
    "    wrapper_clf, \n",
    "    X_test_rfe, \n",
    "    y_test, \n",
    "    scoring='accuracy',\n",
    "    n_repeats=10,\n",
    "    random_state=1     # Fix for reproducibility\n",
    ")\n",
    "\n",
    "# Extract the mean importance values for each feature\n",
    "importances_cls = perm_imp_class.importances_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102fd5d-5efb-4c3e-861e-deabefbfbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3356fd6-3411-465b-9c0b-554829d57c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPlotting Learning Curve...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb841e2-db1a-4f61-911a-13d0359615dd",
   "metadata": {},
   "source": [
    "# 10. Save predictions, selected features with feature importance and PCs to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace22bf5-80a6-49da-873f-d6ab9968ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for Train predictions\n",
    "df_train_pred = pd.DataFrame({\n",
    "    \"ActualLabel\": y_train,\n",
    "    \"PredLabel\": pred_train\n",
    "})\n",
    "\n",
    "# Create DataFrame for Test predictions\n",
    "df_test_pred = pd.DataFrame({\n",
    "    \"ActualLabel\": y_test,\n",
    "    \"PredLabel\": pred_test\n",
    "})\n",
    "    \n",
    "# Create a Dataframe for feature importance\n",
    "selected_feature_names = [feature_names[idx] for idx in selected_features]\n",
    "\n",
    "df_importance = pd.DataFrame({\n",
    "    \"SelectedFeatureIndex\": selected_features,\n",
    "    \"SelectedFeatureName\": selected_feature_names,\n",
    "    \"ClassImportance_Acc\": importances_cls\n",
    "})\n",
    "df_importance_sorted = df_importance.sort_values(\"ClassImportance_Acc\", ascending=False)\n",
    "\n",
    "df_pcs = pd.DataFrame({\n",
    "    \"ActualLabel\": y_full,\n",
    "    \"ConditionName\": pd.Series(y_full).map(condition_names),\n",
    "    \"PC1\": pcs[:, 0],\n",
    "    \"PC2\": pcs[:, 1],\n",
    "    \"PC3\": pcs[:, 2]\n",
    "})\n",
    "\n",
    "# Save all DataFrames to an Excel file with multiple sheets. Version file name to avoid overwriting.\n",
    "output_excel = \"prediction_results_v1.xlsx\"\n",
    "with pd.ExcelWriter(output_excel) as writer:\n",
    "    df_train_pred.to_excel(writer, sheet_name=\"TrainValPredictions\", index=False)\n",
    "    df_test_pred.to_excel(writer, sheet_name=\"TestPredictions\", index=False)\n",
    "    df_importance_sorted.to_excel(writer, sheet_name=\"SelectedFeatures\", index=False)\n",
    "    df_pcs.to_excel(writer, sheet_name=\"PCs\", index=False)\n",
    "\n",
    "print(f\"\\nAll results have been saved to '{output_excel}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
